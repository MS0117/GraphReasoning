# -*- coding: utf-8 -*-
"""gnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uQDOVBnjO0VkPleVdZWJp6RcsjDnMjtJ
"""

import math
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import softmax
from torch_scatter import scatter


def make_one_hot(labels, C):

     
      labels=labels.unsqueeze(1)
      N=labels.size(dim=0)
      one_hot=torch.zeros(N,C,dtype=torch.float).to(labels.device)

      result=one_hot.scatter(1,labels,1).to(labels.device)

      result=Variable(result)

      return result




class GATConvE(MessagePassing):
    """
    Args:
        emb_dim (int): dimensionality of GNN hidden states
        n_ntype (int): number of node types (e.g. 4)
        n_etype (int): number of edge relation types (e.g. 38)
    """
    def __init__(self, emb_dim, n_ntype, n_etype, edge_encoder, head_count=4, aggr="add"):
        super(GATConvE, self).__init__(aggr=aggr)
        self.emb_dim=emb_dim
        self.n_ntype=n_ntype
        self.n_etype=n_etype
        self.edge_encoder=edge_encoder
        self.head_dim=emb_dim//head_count
        self.num_head=head_count

        self.Qlinear=nn.Linear(2*emb_dim,emb_dim)
        self.Mlinear=nn.Linear(3*emb_dim,emb_dim)                               #논문과 다른 부분...논문에서는 2.5인데 왜?
        self.Klinear=nn.Linear(3*emb_dim,emb_dim)

        self._alhpa=None

        self.MLP=nn.Sequential(torch.nn.Linear(emb_dim,emb_dim),torch.nn.BatchNorm1d(emb_dim),torch.nn.ReLU(),torch.nn.Linear(emb_dim,emb_dim))


    def forward(self, x, edge_index, edge_type, node_type, node_feature_extra, return_attention_weights=False):
        """
        x: [N, emb_dim] #[`total_n_nodes`, d_node] (where `total_n_nodes` = b_size * n_node)
        edge_index: [2, E]
        edge_type [E,] -> edge_attr: [E, 39] / self_edge_attr: [N, 39]
        node_type [N,] -> headtail_attr [E, 8(=4+4)] / self_headtail_attr: [N, 8]
        node_feature_extra [N, dim]   #_node_feature_extra = torch.cat([node_type_emb, node_score_emb], dim=2).view(_node_type.size(0), -1).contiguous() #[`total_n_nodes`, dim] type emb와 score에 대한 정보 있음
        """
        N=x.size(0)
        edge=make_one_hot(edge_type,self.n_etype+1)
        self_edge=torch.zeros(N,self.n_etype+1).to(edge.device)
        self_edge[:,self.n_etype]=1                     #마지막 열을 1로 채움

        head_type=node_type[edge_index[0]]          #(E,)
        tail_type=node_type[edge_index[1]]           #(E,)
        head=make_one_hot(head_type,self.n_ntype)       #각 head들의 타입마다 one-hot취함
        tail=make_one_hot(tail_type,self.n_ntype)

        head_tail=torch.cat([head,tail],dim=1)
        self_head=make_one_hot(node_type,self.n_ntype)
        self_headtail=torch.cat([self_head,self_head],dim=1)
        headtail=torch.cat([head_tail,self_headtail],dim=0)         #[E+N,]
        edge_vec=torch.cat([edge,self_edge],dim=0)              #[E+N,]

        edge_embedding=self.edge_encoder(torch.cat([edge_vec,headtail],dim=1))      #[E+N, emb_dim]#########edgeembedding에는 edge type, edge의 head, tail node type info 들어감.

        loop=torch.arange(0,N,dtype=torch.long, device=edge_index.device)
        loop=loop.unsqueeze(0).repeat(2,1)                             #loop_index를 (2,1)만큼 반복      (2,N)
        edge_index=torch.cat([edge_index,loop],dim=1)

        node_info=torch.cat([x,node_feature_extra],dim=1)
        node_info=(node_info,node_info)

        aggr_out=self.propagate(edge_index,x=node_info,edge_attr=edge_embedding)      #[N, emb_dim]
        result=self.MLP(aggr_out)

        alpha=self._alpha

        self._alpha=None

        if return_attention_weights:
          assert alpha is not None
          return result, (edge_index, alpha)               

        else:
          return result                               #[N, emb_dim]   x: [N, emb_dim] #[`total_n_nodes`, d_node] (where `total_n_nodes` = b_size * n_node)      
      

        


    def message(self, edge_index, x_i, x_j, edge_attr): #i: tgt, j:src

        Q=self.Qlinear(x_j).view(-1,self.num_head,self.head_dim)
        K=self.Klinear(torch.cat([x_i,edge_attr],dim=-1)).view(-1,self.num_head,self.head_dim)
        M=self.Klinear(torch.cat([x_j,edge_attr],dim=-1)).view(-1,self.num_head,self.head_dim)

        attention_product=(Q*K).sum(dim=-1) /math.sqrt(self.emb_dim)                                                             #torch.matmul(Q,K.transpose(1,2))/math.sqrt(self.emb_dim) 이거 안 됨..차원이 안 맞음

        attention_weight=softmax(attention_product,edge_index[0])                     #edge_index= (2,E)
        alpha=attention_weight
        source_node_index=edge_index[0]                                       #왜??
        self._alpha=alpha


        edge_num=edge_index.size(1)                                                                         #edge_index는 0부터 오름차순...cid를 n_node로 나눈 나머지값들이 옴.
        node_num=int(edge_index[0].max())+1                                       #?
        ones = torch.full((edge_num,), 1.0, dtype=torch.float).to(edge_index.device)

        src_edge_count=scatter(ones,source_node_index,dim=0, dim_size=node_num, reduce='sum')[source_node_index]
        
        alpha*=src_edge_count.unsqueeze(1)
        out = M * alpha.view(-1, self.num_head, 1) #[E, heads, _dim]
        return out.view(-1, self.emb_dim)  #[E, emb_dim]